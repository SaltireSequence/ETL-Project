{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# San Francisco Datasets (ETL-Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective:**\n",
    "#### The purpose of this project is to collaborate as a team to:\n",
    "* **Extract** - 4 different datasets (in at least 2 formats)\n",
    "* **Transform** - each dataset, based on their current state\n",
    "* **Load** - these non-related datasets as individual datasets as a collection into a NoSQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Resources:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This repo contains the collaborative work for Team 1. In this repo, you will find the following: ####\n",
    "#### **Code Folder** ([link here](https://github.com/SaltireSequence/ETL-Project/tree/master/Code)) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Containing each team member's Jupyter Notebook, that contains their respective code, used for the extraction and transformation of each team member's dataset:\n",
    "\n",
    "| Team Member | Filename | Description |\n",
    "| :- | :- | :- |\n",
    "| William | `william_notebook.ipynb` | Extrating a CSV file and saving as a Pandas DataFrame, containing stats for 150<br> San Francisco restaurants, before undetaking transformation (inc. `str.replace`<br> and code to convert longitute and latitude to zipcodes) and finally converting into a<br>JSON file. |\n",
    "| Caitlin | `SFzips.ipynb` | Caitlin's file, that imports 2x CSV files containing (i) Park (ii) Neighbourhood information,<br> performing transformation (inc. dropping duplicates and deleting obsolete rows), before<br> saving 2x new CSV and finally converting the contents of both CSV files to a JSON file). |\n",
    "| Heesung | `Heesung_code_with_comments.ipynb` | Heesung's file, that imports 2x CSV files (i) Boba Shops in the Bay Area (ii) Pokemon Go<br> Spawns in the Bay Area. Tranformation of the datasets includes (i) removal of irrelevant<br> information, utilizing Geopy to return zipcodes, from latitute and longitude and merging<br> the two datasets using the zipcode as the join. |\n",
    "| Kathryn | `Google_data.ipynb` | Kathryn used Google's API to query of every hotel located within 50,000 meters from the<br> latitude and longitude corresponding to each zip code in the original dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Folder** ([link here](https://github.com/SaltireSequence/ETL-Project/tree/master/Inputs)) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains each team members raw dataset(s), once extraction was completed, but before transformation and loading into the destination database:\n",
    "\n",
    "| Team Member | Filename | Description |\n",
    "| :- | :- | :- |\n",
    "| William | `restaurant_raw.csv` | contains data (inc. longitude and latitude) of 150 Bay Area restaurants.\n",
    "| Caitlin | `Neighborhoods.csv` | contains data for San Francisco parks (inc. a score for each park and<br> it's longitude and latitude)\n",
    "|  | `SFZ.csv` | contains a list of zipcodes for each neighbourhood in San Francisco.\n",
    "| Heesung | `boba.csv` | contains a data for Boba merchants in the Bay Area, which includes each location's longitude and latitude.\n",
    "|  | `pokemon-spawns.csv` | contains data for Pokemon Go spawns in the Bay Area - includes the longitude and latitude of each spawn<br> location, as well as a unique identifier for each Pokemon\n",
    "| Kathryn | **N/A** | As Kathryn queried and API, she has no input file per se."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Outputs Folder**  ([link here](https://github.com/SaltireSequence/ETL-Project/tree/master/Outputs)) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Containing each team member's output files, which includes JSON files (that were subsquently used to form the collections, that made the final MongoDB database) and also any output files, once transformation was completed and before conversion to JSON files:\n",
    "\n",
    "| Team Member | Filename | Description |\n",
    "| :- | :- | :- |\n",
    "| William | `will_data.JSON` | the byproduct of William's data extraction and transformation is a JSON file, used for loading in as the<br> 'Restaurants' collection of the final MongoDB database.\n",
    "| Caitlin | `SF_Park_Scores.csv` | the output files of both of Caitlin's datasets, once transformation had been completed on her original input<br> files, which were `SF_Park_Scores.csv` & `SFZ.csv`.\n",
    "|  | `sf_parks.json` | the output files of both of Caitlin's datasets, once transformation had been completed on her original input<br> files, which were `SF_Park_Scores.csv` & `SFZ.csv`.\n",
    "| Heesung | `pika_boba.csv` | contains a CSV of both datasets, once transformation had completed and before conversion into a JSON file.\n",
    "|  | `boba_pika.json` | contains the boba dataset, once zip codes had bee obtainined, using the longitude and latitude of each boba<br> shop, with help from the `geocoders` library.\n",
    "|  | `boba_zip.csv` | contains the merged dataset data (clean_boba_pika_df) in JSON format, ready for loading to the MongoDB<br> database, as a collection.\n",
    "|  | `pika_zip.csv` | contains the Pokeman dataset, including zip codes for each spawn location.\n",
    "|  | `pika_zip.csv` | contains the Pokeman dataset, including zip codes for each spawn location.\n",
    "| Kathryn | `TBC` | **CONFIRM WITH KATHRYN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Process:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project was to work as a team, as collaboratively as possible, to walkthrough the the ETL process with at least 2 datasets, from different sources (e.g Kaggle & API, in atleast 2 differeing formats (e.g JSON and CSV).\n",
    "\n",
    "The obvious approach would to have had individual team members tackle a different stage of the ETL process, however it was collectively agreed that this approach wouldn't necessarily have the entire team empowered with the knowledge of every step of the ETL process.\n",
    "\n",
    "Consensus was therefore reached, that each team member would go through the ETL process individually, but in parallel. **Each team member would therefore:**\n",
    "\n",
    "- **Extract** at least 1x dataset\n",
    "- Perform **transformation**, dependant on the type and state of the data, following extraction. The prerequisite being, that each team member would present their transformed data in JSON format.\n",
    "- Individually **load** their JSON-formatted transformed dataset(s) into a personal NOSQL (MongoDB) data, as a sanity check, before...\n",
    "- Submitting their JSON file (that has successfully parsed by MongoDB) to the team member's respective resources file.\n",
    "\n",
    "Once each team member had successfully extracted, transformed and 'test' loaded their datasets into MongoDB, Kathryn then took the helm of the load element of the project, by uploading each team member's dataset JSON file as individual collections in a master MongoDB database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extraction** ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total the team extracted 6 datasets, from 3 sources, in 2 formats:\n",
    "\n",
    " - **6 datasets:** stored in the [Input Folder](https://github.com/SaltireSequence/ETL-Project/tree/master/Inputs)\n",
    " - **3 sources:** we used [Kaggle](https://www.kaggle.com) and the [Google API](https://maps.googleapis.com/)\n",
    " - **2 formats:** William, Heesung and Cailin's data all derived in CSV format, whilst Kathryn's API call to Goole, returned a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
